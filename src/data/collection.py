import requests
import pandas as pd
from io import StringIO
from pathlib import Path
import time
from datetime import datetime
import os
from dotenv import load_dotenv

# --- Configuration ---
# Load environment variables from .env file
load_dotenv()

# --- API Keys ---
# Ensure these are set in your .env file in the project root:
API_KEYS = [
    os.getenv("LOGICAL_TOKEN"),
    os.getenv("FISK_TOKEN")
]
API_KEYS = [key for key in API_KEYS if key] # Filter out None values if a token isn't set

if not API_KEYS:
    raise ValueError("No API tokens found in .env file. Please set LOGICAL_TOKEN and/or FISK_TOKEN.")

current_key_index = 0
current_key_name = ["LOGICAL_TOKEN", "FISK_TOKEN"] # For logging which key is active

BASE_DATA_URL = "https://www.ncei.noaa.gov/access/services/data/v1"

# --- File Paths ---
# Paths are relative to the project root (assuming script is run from project root)
GSN_STATIONS_CSV = Path("data") / "interim" / "gsn_stations.csv"
RAW_DATA_OUTPUT_DIR = Path("data") / "raw" / "gsoy_data" # Subdirectory for GSOY raw data

PROCESSED_STATIONS_LOG = Path("data") / "raw" / "processed_gsoy_stations.log"
ERROR_LOG = Path("data") / "raw" / "gsoy_download_errors.log"

# --- Data Range & Types ---
START_YEAR = 1700 
END_YEAR = datetime.now().year # Current year
API_START_DATE = f"{START_YEAR}-01-01"
API_END_DATE = f"{END_YEAR}-12-31"

# The full list of data types confirmed by our successful test run
ALL_REQUESTED_BASE_DATATYPES = [
    'CDSD', 'CLDD', 'DP01', 'DP10', 'DT00', 'DT32', 'DX32', 'DX70', 'DX90',
    'EMNT', 'EMXP', 'EMXT', 'FZF0', 'FZF5', 'FZF6', 'HDSD', 'HTDD', 'PRCP',
    'TAVG', 'TMAX', 'TMIN'
]

# API Parameters that remain constant across requests (station-specific params added later)
API_PARAMS_BASE = {
    "dataset": "global-summary-of-the-year",
    "startDate": API_START_DATE,
    "endDate": API_END_DATE,
    "dataTypes": ",".join(ALL_REQUESTED_BASE_DATATYPES),
    "format": "csv",
    "units": "metric",
    "includeStationName": "true",
    "includeStationLocation": "true",
    "includeAttributes": "true" # Request all _ATTRIBUTES
}

# --- Retry & Rate Limit Settings ---
REQUEST_DELAY_SECONDS = 0.3 
RETRY_DELAY_ON_SERVER_ERROR = 60 # Seconds to wait on 503 error
MAX_RETRIES_PER_KEY = 3 
MAX_KEY_SWITCH_ATTEMPTS = len(API_KEYS) 


# --- Helper Functions ---

def get_current_api_token():
    """Returns the current active API token."""
    return API_KEYS[current_key_index]

def get_current_key_name():
    """Returns the name of the current active API key."""
    return current_key_name[current_key_index]

def switch_api_key():
    """Switches to the next API key in the list."""
    global current_key_index
    if len(API_KEYS) > 1 and current_key_index < len(API_KEYS) - 1:
        current_key_index += 1
        print(f"\n--- Switched to API Key: {get_current_key_name()} ---")
        return True
    return False

def load_processed_stations(log_file_path):
    """Loads station IDs from the log file to resume downloads."""
    if log_file_path.exists():
        with open(log_file_path, 'r') as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def log_processed_station(log_file_path, station_id):
    """Logs a successfully processed station ID."""
    with open(log_file_path, 'a') as f:
        f.write(f"{station_id}\n")

def log_error(error_log_path, station_id, error_message):
    """Logs an error for a specific station."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(error_log_path, 'a') as f:
        f.write(f"[{timestamp}] Station: {station_id}, Error: {error_message}\n")


# --- Main Data Collection Logic ---
def collect_gsoy_data():
    """
    Collects Global Summary of the Year (GSOY) data for GSN stations.
    Implements robust error handling, progress tracking, and API key switching.
    """
    RAW_DATA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    PROCESSED_STATIONS_LOG.parent.mkdir(parents=True, exist_ok=True) # Ensure parent dir for logs
    ERROR_LOG.parent.mkdir(parents=True, exist_ok=True) # Ensure parent dir for errors

    if not GSN_STATIONS_CSV.exists():
        print(f"Error: GSN stations file '{GSN_STATIONS_CSV}' not found.")
        print("Please ensure this file is generated by previous processing scripts.")
        return

    df_gsn_stations = pd.read_csv(GSN_STATIONS_CSV)
    print(f"Loaded {len(df_gsn_stations)} GSN stations for data collection.")

    processed_stations = load_processed_stations(PROCESSED_STATIONS_LOG)
    print(f"Found {len(processed_stations)} stations already processed. Starting from where we left off...")

    total_downloads = 0
    skipped_count = 0
    error_count = 0
    
    # Track which stations failed to avoid redundant error logging in one run
    current_run_failed_stations = set() 

    for index, row in df_gsn_stations.iterrows():
        station_id = row['ID']
        country_name = str(row['COUNTRY_NAME']) # Convert to string to handle potential NaNs safely
        station_name = str(row['NAME'])

        if station_id in processed_stations:
            skipped_count += 1
            continue # Skip already processed station

        if station_id in current_run_failed_stations:
            continue

        print(f"\nAttempting to download data for {station_id} ({station_name}, {country_name})...")

        # Prepare parameters for the current station
        params = API_PARAMS_BASE.copy()
        params["stations"] = station_id

        # Clean country name for directory (replace spaces/slashes with underscores)
        cleaned_country_name = country_name.replace(" ", "_").replace("/", "_").replace("\\", "_")
        if not cleaned_country_name: # Handle case where country_name might be empty/NaN
            cleaned_country_name = "Unknown_Country"

        country_output_path = RAW_DATA_OUTPUT_DIR / cleaned_country_name
        country_output_path.mkdir(parents=True, exist_ok=True)
        
        # Define the output file path for this station
        output_file_name = f"{station_id}_GSOY_{START_YEAR}-{END_YEAR}.csv"
        output_file_path = country_output_path / output_file_name

        key_switch_attempt_count = 0
        success = False

        while not success and key_switch_attempt_count < MAX_KEY_SWITCH_ATTEMPTS:
            retries_with_current_key = 0
            
            while retries_with_current_key < MAX_RETRIES_PER_KEY:
                try:
                    # Set API token in headers for current request
                    headers = {"token": get_current_api_token()}
                    response = requests.get(BASE_DATA_URL, params=params, headers=headers)
                    response.raise_for_status() # Raises HTTPError for 4xx or 5xx responses

                    # If successful (status 200)
                    data = response.text
                    if not data.strip(): # Check if response body is empty
                        print(f"  Warning: No data returned for {station_id} ({country_name}) - Empty response.")
                        log_error(ERROR_LOG, station_id, "Empty response from API.")
                        error_count += 1
                        current_run_failed_stations.add(station_id)
                        success = True # Treat as "handled", no need to retry or switch key for this specific empty response
                        break 
                    
                    # Check if the response is just the CSV header without data rows
                    # This often happens if dataTypes are specified but no data is available for them
                    if len(data.splitlines()) <= 1 and "STATION" in data.splitlines()[0]:
                        print(f"  Warning: Only header returned for {station_id} ({country_name}). No data for specified types/period.")
                        log_error(ERROR_LOG, station_id, "Only CSV header returned (no data rows for specified types/period).")
                        error_count += 1
                        current_run_failed_stations.add(station_id)
                        success = True # Treat as handled, no more retries
                        break

                    # Process and save data
                    df_station_data = pd.read_csv(StringIO(data))
                    
                    df_station_data['Source_Station_ID'] = station_id
                    df_station_data['Source_Country_Name'] = country_name
                    df_station_data['Source_Station_Name'] = station_name
                    
                    df_station_data.to_csv(output_file_path, index=False)
                    log_processed_station(PROCESSED_STATIONS_LOG, station_id)
                    total_downloads += 1
                    print(f"   Downloaded {len(df_station_data)} rows. Saved to {output_file_path}")
                    success = True # Mark as successful, break out of retry loops
                    break

                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 403: # Forbidden/Rate Limit Exceeded
                        print(f"  Error {station_id}: API Key Rate Limit Exceeded or Forbidden (403).")
                        log_error(ERROR_LOG, station_id, f"403 Forbidden - Rate limit hit or invalid token (Key: {get_current_key_name()}).")
                        retries_with_current_key += 1
                        if retries_with_current_key >= MAX_RETRIES_PER_KEY:
                            print(f"    Max retries ({MAX_RETRIES_PER_KEY}) for current key {get_current_key_name()} reached. Attempting to switch key.")
                            key_switch_attempt_count += 1
                            if switch_api_key():
                                # Switched, reset retries for new key, continue outer loop
                                retries_with_current_key = 0
                                time.sleep(RETRY_DELAY_ON_SERVER_ERROR) # Wait before using new key
                                continue # Continue with new key
                            else:
                                print("    No more API keys available. Aborting downloads.")
                                log_error(ERROR_LOG, station_id, "All API keys exhausted or failed.")
                                error_count += 1
                                current_run_failed_stations.add(station_id)
                                return # Exit function if no more keys
                        else:
                            print(f"    Retrying with current key {get_current_api_token()}. Attempt {retries_with_current_key}/{MAX_RETRIES_PER_KEY}.")
                            time.sleep(REQUEST_DELAY_SECONDS * 5) # Longer delay before retrying same key
                            continue # Retry current key

                    elif e.response.status_code == 503: # Service Unavailable
                        print(f"   Error {station_id}: Server temporarily unavailable (503). Retrying after {RETRY_DELAY_ON_SERVER_ERROR}s.")
                        log_error(ERROR_LOG, station_id, f"503 Service Unavailable (Key: {get_current_key_name()}).")
                        retries_with_current_key += 1
                        time.sleep(RETRY_DELAY_ON_SERVER_ERROR) # Wait significantly for server to recover
                        continue # Retry with current key

                    else: # Other HTTP Errors (e.g., 400 Bad Request, 404 Not Found)
                        error_message = f"HTTP Error {e.response.status_code}: {e.response.text.strip()[:200]}..."
                        print(f"   Error {station_id}: {error_message}")
                        log_error(ERROR_LOG, station_id, error_message)
                        error_count += 1
                        current_run_failed_stations.add(station_id)
                        success = True # Treat as handled, don't retry or switch key for this type of error
                        break

                except requests.exceptions.RequestException as e:
                    error_message = f"Network/Request Error: {e}"
                    print(f"   Network Error for {station_id}: {error_message}. Retrying...")
                    log_error(ERROR_LOG, station_id, error_message)
                    retries_with_current_key += 1
                    time.sleep(REQUEST_DELAY_SECONDS * 10) # Longer delay for network issues
                    continue # Retry with current key
            
            if not success and key_switch_attempt_count >= MAX_KEY_SWITCH_ATTEMPTS:
                print(f"    Failed after trying all API keys for station {station_id}. Moving to next station.")
                log_error(ERROR_LOG, station_id, "Failed after trying all API keys.")
                error_count += 1
                current_run_failed_stations.add(station_id)
                break # Break out of key-switch loop, move to next station

        time.sleep(REQUEST_DELAY_SECONDS) # Standard delay between requests

    print("\n--- Data Collection Summary ---")
    print(f"Total GSN stations in list: {len(df_gsn_stations)}")
    print(f"Stations skipped (already processed): {skipped_count}")
    print(f"New stations downloaded successfully: {total_downloads}")
    print(f"Stations with permanent/unresolved errors: {error_count} (check '{ERROR_LOG}')")
    print(f"Raw data stored in: '{RAW_DATA_OUTPUT_DIR}'")

# --- Main execution block ---
if __name__ == "__main__":
    # Ensure this script is run from the project root directory (e.g., gcrp-global-climate-risk-prediction)
    print("Starting GSOY data collection process...")
    collect_gsoy_data()